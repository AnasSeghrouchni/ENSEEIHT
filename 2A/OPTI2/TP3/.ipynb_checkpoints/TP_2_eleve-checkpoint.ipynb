{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Méthodes d'optimisation stochastique\n",
    "\n",
    "## I. Minimisation stochastique d'une fonction déterministe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "On s'intéresse au problème $\\min f(x) =\\max_{i=1\\dots m}|a_i^Tx-b_i | = \\max_{i=1\\dots m} |(Ax-b)_i|.$ \n",
    "\n",
    "On suppose que $a_i$ est un vecteur colonne représentant la $i$eme ligne d'une matrice $A$ de taille $m \\times n$ ($m=100, n=20$) , et les $b_i$ sont les composantes d'un second membre $b$ de taille $m$, ($1 \\le i \\le m$), $x$ un vecteur de taille $n$. Ces quantités sont générées une fois pour toutes à partir de distributions Gaussiennes de moyenne nulle et d'écart type identité.\n",
    "\n",
    "**Question 1 :** Constuire $A$ et $b$. Proposer le calcul d'un sous-gradient en $x$ de $f$. On pourra utiliser la fonction findmax de Julia. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `findmax` not found.\n"
     ]
    }
   ],
   "source": [
    "?findmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3812247323.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    using Random\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "using Statistics\n",
    "# Construction des données A  et b\n",
    "# Insérer votre code\n",
    "\n",
    "m = 100;\n",
    "n = 20;\n",
    "A = randn(m,n);\n",
    "b = randn(m,1);\n",
    "x = randn(n,1)\n",
    "# Fin insérer code\n",
    "\n",
    "# Fonction calculant un sous-gradient en x de f\n",
    "function subgrad(A,b,x) \n",
    "    # Insérer votre code\n",
    "    fx = A*x - b;\n",
    "    (max,ind) = findmax(abs.(fx));\n",
    "    i = ind[1]\n",
    "    return (sign(fx[i]))*A[i,:];\n",
    "    # Fin insérer code\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 :** Ce problème peut se reformuler comme un problème de programmation linéaire : $$(\\mathcal{P}_{lp})\\quad \\left\\{ \\begin{array}{c} \\min_{(x,R)\\in \\mathbb{R}^n\\times \\mathbb{R}} h(x,R)=R\\\\\n",
    "s.c. \\quad-R*e\\leq A*x-b\\leq R*e\\end{array}\\right.$$ avec $e=[1,\\cdots,1]^T\\in \\mathbb{R}^m$. Résoudre le problème $(\\mathcal{P}_{lp})$ en utilisant le solveur \"GLPK\" de la librairie JuMP. Plus d'informations sont disponibles ici :  http://www.juliaopt.org/JuMP.jl/latest/quickstart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using GLPK\n",
    "\n",
    "# Définition du modèle\n",
    "# Insérer votre code\n",
    "\n",
    "model = Model(GLPK.Optimizer)\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition des variables d'optimisation\n",
    "# Insérer votre code\n",
    "@variable(model, x[1:n]);\n",
    "@variable(model, R);\n",
    "\n",
    "\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition de la fonctionnelle à minimiser\n",
    "#Insérer votre code\n",
    "\n",
    "@objective(model, Min, R);\n",
    "\n",
    "# Fin insérer code\n",
    "\n",
    "# Définition des contraintes\n",
    "# Insérer votre code\n",
    "\n",
    "@constraint(model, c1[i=1:m], R >= (sum(A[i,j]*x[j] for j in 1:n) - b[i]));\n",
    "@constraint(model, c2[i=1:m], -R <= (sum(A[i,j]*x[j] for j in 1:n) - b[i]));\n",
    "\n",
    "#for i=1:m\n",
    "#    @constraint(model, sum(A[i,j]*x[j] for j in 1:n) - b[i] - R <= 0);\n",
    "#    @constraint(model, sum(A[i,j]*x[j] for j in 1:n) - b[i] + R >= 0);\n",
    "#end\n",
    "# Fin insérer code\n",
    "        \n",
    "# Résolution        \n",
    "# Insérer votre code\n",
    "\n",
    "optimize!(model);\n",
    "\n",
    "\n",
    "# Fin insérer code\n",
    "        \n",
    "# Résultats à optimalité                \n",
    "# Insérer votre code\n",
    "xstar = value.(x); \n",
    "Rstar = value.(R);\n",
    "\n",
    "# Fin insérer code\n",
    "println(\"The function value at the solution is: \",Rstar, \" or \",findmax(abs.(A*xstar-b))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 :** Résoudre le problème en utilisant un algorithme de sous-gradient. Dans un premier temps vous utiliserez un sous-gradient exact (Question 1), puis vous introduirez un bruit artificiel qui suit une distribution normale de moyenne nulle et d'écart-type $3 e-1$.\n",
    "\n",
    "**Question 4 :** Vous afficherez les courbes de convergence de $f_{best}^k-f_{star}$, avec $f_{star}$ obtenue à la Question 1. Donnez la valeur minimale de $f_{best}^k-f_{star}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "using Plots\n",
    "\n",
    "#Initialisation\n",
    "x = zeros(n,1);\n",
    "i = 0;\n",
    "fbest =1e10; # $f_{best}^0$: cas du sous-gradient exact\n",
    "fbestp=1e10; # $f_{best}^0$: cas du sous-gradient bruité\n",
    "histo =[];# Suite des itérés f_{best}^k-f_{star} pour le cas du sous-gradient exact\n",
    "histop=[];# Suite des itérés f_{best}^k-f_{star}, pour le cas du sous-gradient bruité\n",
    "\n",
    "#Niveau de bruit\n",
    "noise_lvl=.3;\n",
    "\n",
    "# Resolution \n",
    "# Insérer votre code\n",
    "itermax = 100\n",
    "xp = x\n",
    "xp_b = x\n",
    "while i < itermax\n",
    "    i = i + 1\n",
    "    ak = 1/(i)\n",
    "    xp = xp - ak*subgrad(A,b,xp)\n",
    "    fxp = findmax(abs.(A*xp-b))[1]   \n",
    "    if (fxp < fbest) \n",
    "        fbest = fxp\n",
    "    end\n",
    "    xp_b = xp_b - ak*(subgrad(A,b,xp_b) + noise_lvl*randn(n,1));\n",
    "    fxpb = findmax(abs.(A*xp_b-b))[1]   \n",
    "    if (fxpb < fbestp) \n",
    "        fbestp = fxpb\n",
    "    end\n",
    "    append!(histo, fbest-Rstar)\n",
    "    append!(histop, fbestp-Rstar)\n",
    "end\n",
    "# Fin insérer code\n",
    "\n",
    "#Affichage des courbes de convergence\n",
    "iter=1:100;\n",
    "hf=[histo,histop];\n",
    "print(\"La meilleur valeur de f_best-R_star est: \")\n",
    "print('\\n')\n",
    "println(\"Sans bruit: \" * string(histo[100]))\n",
    "println(\"Bruité: \" * string(histop[100]))\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Exact\" \"Noisy\"],lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## II. Minimisation stochastique d'une fonction stochatique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "On s'intéresse au problème\n",
    "$$\\min_x f(x) =\\text{E} (\\max_{i=1\\dots m}|a_i^Tx-b_i |).$$\n",
    "\n",
    "On suppose que $a_i$ est un vecteur colonne représentant la $i$eme ligne d'une matrice $A$ de taille $m \\times n$ ($m=100, n=20$) , et les $b_i$ sont les composantes d'un second membre $b$ de taille $m$, ($1 \\le i \\le m$), $x$ un vecteur de taille $n$. Ces quantités sont générées une fois pour toutes à partir de distributions Gaussiennes de moyenne connue $\\bar{A}$ et $\\bar{b}$ (non nécesairement nulle) et d'écart type identité.\n",
    "\n",
    "**Question 5 :** Proposer deux fonctions d'évaluation de la fonction $f$ et d'un sous-gradient de $f$ basées sur des échantillons de taille $M$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation de f\n",
    "function fvals(Abar,bbar,noise,xs,M) \n",
    "    # Abar, bbar : moyenne des données\n",
    "    # noise : niveau de bruit \n",
    "    # xs : vecteur courant \n",
    "    # M: taille de l'échantillon\n",
    "\n",
    "# Insérer votre code\n",
    "    s = 0\n",
    "    for i=1:M\n",
    "        A = noise*randn(m,n) + Abar\n",
    "        b = noise*randn(m,1) + bbar\n",
    "        s += findmax(abs.(A*xs-b))[1]\n",
    "    end\n",
    "    return s/M\n",
    "# Fin insérer code\n",
    "\n",
    "end\n",
    "\n",
    "# Evaluation d'un sous-gradient\n",
    "function subgrads(Abar,bbar,noise,xs,M)\n",
    "    # Abar, bbar : moyenne des données\n",
    "    # noise : niveau de bruit \n",
    "    # xs : vecteur courant \n",
    "    # M: taille de l'échantillon\n",
    "    \n",
    "# Insérer votre code\n",
    "\n",
    "    s = zeros(n,1)\n",
    "    for i = 1:M\n",
    "        A = noise*randn(m,n) + Abar\n",
    "        b = noise*randn(m,1) + bbar\n",
    "        s = s + subgrad(A,b,xs)\n",
    "    end\n",
    "    return s/M\n",
    "# Fin insérer code\n",
    "\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 6 :** Comparer les courbes de convergence du problème déterministe $$ \\min_x f(x) = \\max_{i=1\\dots m}|\\text{E} (a_i)^Tx-\\text{E} (b_i) |,$$ et du problème stochastique obtenu avec $M=10,100,1000$ échantillons. Donnez la valeur minimale de $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Données\n",
    "Abar=10*ones(m,n)+1*randn(m,n);\n",
    "bbar=10*randn(m,1);\n",
    "\n",
    "#x_0\n",
    "xd = zeros(n,1); # problème déterministe\n",
    "xs = xd; # problème stochastique\n",
    "\n",
    "# Bruit et echantillon\n",
    "M  = 100;\n",
    "noise  = 4;\n",
    "\n",
    "i = 0;\n",
    "\n",
    "fbestd =1e10; # $f_{best}^0$: cas d'une résolution déterministe\n",
    "fbests =1e10; # $f_{best}^0$: cas d'une résolution stochastique\n",
    "histod =[]; # Suite des itérés f_{best}^k pour le cas d'une résolution déterministe\n",
    "histos =[]; # Suite des itérés f_{best}^k pour le cas d'une résolution stochastique\n",
    "\n",
    "# Insérer votre code\n",
    "\n",
    "itermax = 100\n",
    "while i < itermax\n",
    "    i = i + 1\n",
    "    ak = 1/i\n",
    "    #resolution deterministe\n",
    "    xd = xd - ak*subgrad(Abar,bbar,xd);\n",
    "    fxd = fvals(Abar,bbar,noise,xd,M)  \n",
    "    if (fxd < fbestd) \n",
    "        fbestd = fxd\n",
    "    end\n",
    "    append!(histod, fbestd)\n",
    "    #résolution stochastique\n",
    "    xs = xs - ak*(subgrads(Abar,bbar,noise,xs,M))\n",
    "    fxs = fvals(Abar,bbar,noise,xs,M) \n",
    "    if (fxs < fbests)\n",
    "        fbests = fxs\n",
    "    end\n",
    "    append!(histos, fbests)\n",
    "end\n",
    "\n",
    "# Fin insérer code\n",
    "#Affichage\n",
    "\n",
    "iter=1:100;\n",
    "hf=[histod,histos];\n",
    "print(\"La meilleur valeur de f_best est: \")\n",
    "print('\\n')\n",
    "println(\"Deterministe: \" * string(histod[100]))\n",
    "println(\"Stochastique: \" * string(histos[100]))\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Deterministic\" \"Stochastic\"],lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la convergence de la méthode stochastique est proportionelle à la taille de l'enchantillon. Ainsi la méthode stochastique converge vers une meilleure valeur, pour un M grand, que la méthode déterministe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question 7 :** Répéter les expériences et comparer les valeurs meilleurs valeurs de f obtenues ($f_{best}$) aprs un nombre fixé d'itérations. Donnez la valeur minimale de $f_{best}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Données\n",
    "Abar=10*ones(m,n)+1*randn(m,n);\n",
    "bbar=10*randn(m,1);\n",
    "\n",
    "# x_0\n",
    "xd = zeros(n,1); # résolution déterministe\n",
    "xs = xd;  # résolution stochastique\n",
    "\n",
    "# Bruit et echantillon\n",
    "M  = 100;\n",
    "noise  = 4;\n",
    "\n",
    "#Nombre d'itérations\n",
    "niter=100;\n",
    "\n",
    "j = 0;\n",
    "\n",
    "\n",
    "fbesttd=[]; # f_{best} pour chaque expériences dans le cas d'une résolution déterministe\n",
    "fbestts=[]; # f_{best} pour chaque expériences dans le cas d'une résolution stochastique\n",
    "\n",
    "for nexp=1:20\n",
    "    # Répétition des expériences\n",
    "    fbestd =1e10; # $f_{best}^0$: cas d'une résolution déterministe\n",
    "    fbests =1e10; # $f_{best}^0$: cas d'une résolution stochastique    \n",
    "    # Insérer votre code\n",
    "    itermax = niter\n",
    "    xd = zeros(n,1); \n",
    "    xs = xd;\n",
    "    i=0\n",
    "    while i < itermax\n",
    "        i = i + 1\n",
    "        ak = 1/(i)\n",
    "        #resolution deterministe\n",
    "        xd = xd - ak*subgrad(Abar,bbar,xd);\n",
    "        fxd = fvals(Abar,bbar,noise,xd,M)   \n",
    "        if (fxd < fbestd) \n",
    "            fbestd = fxd\n",
    "        end\n",
    "        #résolution stochastique\n",
    "        xs = xs - ak*(subgrads(Abar,bbar,noise,xs,M))\n",
    "        fxs = fvals(Abar,bbar,noise,xs,M) \n",
    "        if (fxs < fbests)\n",
    "            fbests = fxs\n",
    "        end\n",
    "    end\n",
    "    append!(fbesttd,fbestd)\n",
    "    append!(fbestts,fbests)\n",
    "# Fin insérer code\n",
    "end\n",
    "\n",
    "#Affichage\n",
    "\n",
    "iter=1:20;\n",
    "hf=[fbesttd,fbestts];\n",
    "\n",
    "plot(iter,hf,title=\"Convergence curves\",label=[\"Deterministic\" \"Stochastic\"],lw=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La meilleur valeur de f_best après 20 itérations est: \")\n",
    "print('\\n')\n",
    "println(\"Deterministe: \" * string(findmin(fbesttd)[1]))\n",
    "println(\"Stochastique: \" * string(findmin(fbestts)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il y a un gap entre la méthode stochastique et la méthode déterministe. Ainsi l'approximation faite à la question 6 ne permet pas d'approcher rellement le minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
