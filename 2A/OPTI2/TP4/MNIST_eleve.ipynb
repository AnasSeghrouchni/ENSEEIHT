{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Optimisation II:  A simple neural network \n",
    "\n",
    "### Nom(s): \n",
    "### Pr√©nom(s): \n",
    "### Groupe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  60000 images in the train set\n",
      "There are  10000 images in the test set\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Load train data\n",
    "#\n",
    "Xtrain = np.load('train-images.npy')\n",
    "Xtrain = np.array([x.ravel()/255 for x in Xtrain])\n",
    "Xtrain = Xtrain.reshape(Xtrain.shape[0],Xtrain.shape[1],1)\n",
    "Ytrain = np.load('train-labels.npy')\n",
    "targets_train = []\n",
    "\n",
    "#\n",
    "# Convert digits to 10x1 vectors\n",
    "#\n",
    "for lab in Ytrain:\n",
    "    v      = np.zeros((10,1))\n",
    "    v[lab] = 1\n",
    "    targets_train+=[np.array(v)]\n",
    "\n",
    "#\n",
    "# Load test data\n",
    "#\n",
    "Xtest        = np.load('t10k-images.npy')\n",
    "Xtest        = np.array([x.ravel()/255 for x in Xtest])\n",
    "Xtest        = Xtest.reshape(Xtest.shape[0],Xtest.shape[1],1)\n",
    "Ytest        = np.load('t10k-labels.npy')\n",
    "targets_test = []\n",
    "\n",
    "#\n",
    "# Convert digits to 10x1 vectors\n",
    "#\n",
    "for lab in Ytest:\n",
    "    v = np.zeros((10,1))\n",
    "    v[lab]=1\n",
    "    targets_test+=[np.array(v)]\n",
    "#\n",
    "# Outputs\n",
    "#\n",
    "print('There are ',Xtrain.shape[0],'images in the train set')\n",
    "print('There are ',Xtest.shape[0],'images in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the activation function\n",
    "\n",
    " The activation function defines the output of a node given a set of inputs. We use the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax</a> function defined by\n",
    " \n",
    " $$\\sigma_{\\alpha} : \\mathbb{R}^p\\rightarrow [0,1]^p, \\quad \\mbox{ s.t.} \\quad[\\sigma_{\\alpha}(x)]_i=\\frac{e^{x_i+\\alpha_i}}{\\displaystyle{\\sum_{j=1}^{p}e^{x_j+\\alpha_j}}}\\quad \\forall i=1:p. $$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Nonlinear activation function\n",
    "#\n",
    "def softmax(x,alpha):\n",
    "    \"\"\"\n",
    "    Softmax unit activation function \n",
    "    x    : Numpy array\n",
    "    alpha: scalar\n",
    "    \"\"\" \n",
    "    p = len(x)\n",
    "    value = np.zeros((p,1))\n",
    "    for i in range(p):\n",
    "        value[i] = np.exp(x[i] + alpha[i]) # value[i] = np.exp(x[i] + alpha)\n",
    "    value = (value/sum(value))\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Example of a plot of the activation function\n",
    "#\n",
    "t     = np.arange(-5,5,0.1)\n",
    "alpha = np.arange(-50,50,1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(t,softmax(t,alpha))\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of a simple neural network\n",
    "\n",
    "We use a one-layer fully-connected neural network with the <a href=\"https://en.wikipedia.org/wiki/Softmax_function\">softmax</a> activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(x,W):\n",
    "    \"\"\"\n",
    "    # One-layer fully connected neural network\n",
    "    # x: image, i.e. 784x1 vector (28x28)\n",
    "    # W: weight matrices of shape 10x784   \n",
    "    \"\"\"\n",
    "    p = W.shape[0]\n",
    "    prod = W@x\n",
    "    pred = softmax(prod, np.zeros((p,1)))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the loss function\n",
    "\n",
    "The loss function is the <a href=\"https://en.wikipedia.org/wiki/Cross_entropy\">cross-entropy</a> defined by \n",
    "\n",
    "$$J(W)=-\\sum_{i=1}^N p_i \\log(q_i(W)),$$ where $N$ is the number of classes, $(p_i)_{i=1:N}$ are the probabilities of  a data from the training set to belong to a class (0 or 1 because the labels are known), and $(q_i(W))_{i=1:N}$ are the predicted probabilities from the model\n",
    "\n",
    "$$\\forall i=1:N, \\quad q_i(W)=[\\sigma_{\\alpha}(Wx)]_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Loss function = Cross-entropy\n",
    "#\n",
    "def cross_entropy(pred,target,x):\n",
    "    \"\"\"\n",
    "    pred:   predicted probabilities (q(W))\n",
    "    target: probabilities (p)\n",
    "    x:      image \n",
    "    \"\"\" \n",
    "    #\n",
    "    # TO DO: return ce (cross_entropy)\n",
    "    #\n",
    "    ce = -sum(np.transpose(target)@np.log(pred))\n",
    "    #\n",
    "    # gradient of the cross-entropy\n",
    "    s = pred\n",
    "    u    =  np.ones((s.shape[0],1))\n",
    "    g    = (-u.T.dot(target)*(s.dot(x.T))+target*x.T)\n",
    "    grad = -g.reshape(x.shape[0]*target.shape[0],1)\n",
    "    \n",
    "    return ce,grad\n",
    "#\n",
    "# Main function \n",
    "#\n",
    "def f(W,x,target):\n",
    "    \"\"\"\n",
    "    W:      weights\n",
    "    target: probabilities (p)\n",
    "    x:      image\n",
    "    \"\"\"\n",
    "    #\n",
    "    # TO DO: return ce, grad, pred (cross_entropy, gradient, predicted probabilities)\n",
    "    #\n",
    "    pred = NN(x,W) \n",
    "    ce,grad = cross_entropy(pred,target,x)\n",
    "    \n",
    "    return ce,grad,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.22456072]\n",
      "[[1.22456125]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Test information on the gradient with calls of f\n",
    "#\n",
    "\n",
    "# Define weight matrices\n",
    "W      = np.random.rand(10,Xtrain.shape[1])\n",
    "eps    = 1e-8\n",
    "d      = np.random.rand(10,Xtrain.shape[1])\n",
    "Wtilde = W+eps*d\n",
    "\n",
    "# Retrieve the information on the gradients\n",
    "res    = (f(Wtilde,Xtrain[0],targets_train[0])[0]-f(W,Xtrain[0],targets_train[0])[0])/eps\n",
    "print(res)\n",
    "\n",
    "g      = f(W,Xtrain[0],targets_train[0])[1]\n",
    "print(g.T.dot(d.reshape(7840,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Function to create batches of samples to be used later in the training phase\n",
    "#\n",
    "def create_batches(x,bs):\n",
    "    \"\"\"\n",
    "    x : set to be considered (array)\n",
    "    bs: batch size (scalar)\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    ind     = np.arange(x.shape[0])\n",
    "    random.shuffle(ind)\n",
    "    nbatch  = ind.shape[0]//bs\n",
    "    rest    = ind.shape[0]%bs\n",
    "    \n",
    "    for n in range(nbatch):\n",
    "        batches +=[ind[bs*n:bs*(n+1)]]\n",
    "    \n",
    "    # Put the remaining elements in a last batch\n",
    "    if rest !=0:        \n",
    "        batches += [ind[-rest:]]\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history    = {}\n",
    "eta        = 1e-5 # learning rate\n",
    "momentum   = 0.   # momemtum factor\n",
    "N_EPOCHS   = 10  \n",
    "BatchSizes = [10000,1024,256] # try different batch sizes for the analysis\n",
    "\n",
    "for bs in BatchSizes:\n",
    "    #\n",
    "    # Sensitivity to the batch size to be investigated in the analysis\n",
    "    #\n",
    "    print('batch size=',bs)\n",
    "    \n",
    "    history[bs]={'train loss':[],'train acc':[],'test loss':[0], 'test acc':[0]}\n",
    "    \n",
    "    # Initialization of the weights\n",
    "    w = np.random.rand(10,Xtrain.shape[1])\n",
    "    \n",
    "    for n in range(N_EPOCHS):\n",
    "        # Minimization of the loss function\n",
    "        \n",
    "        Batches=create_batches(Xtrain,bs)\n",
    "        \n",
    "        for batch in Batches:\n",
    "            # Loop on the batches\n",
    "            ce_sum = 0\n",
    "            grad_sum = 0\n",
    "            train_acc = 0\n",
    "            for i in batch:\n",
    "                x_t = Xtrain[i]\n",
    "                target_t = targets_train[i]\n",
    "                W = w\n",
    "                ce,grad,pred = f(W,x_t,target_t)\n",
    "                \n",
    "                ce_sum += ce\n",
    "                grad_sum += grad\n",
    "                \n",
    "                #on regarde si la prediction vaut bien le label de notre cible\n",
    "                if (list(pred).index(np.max(pred)) == list(target_t).index(1)):\n",
    "                    train_acc += 1\n",
    "                \n",
    "            history[bs]['train acc'] += [train_acc/batch.shape[0]]\n",
    "            history[bs]['train loss'] += [ce_sum/batch.shape[0]]\n",
    "            \n",
    "            #modication des poids\n",
    "            sum_grad = grad_sum.reshape(W.shape[0],W.shape[1])\n",
    "            w = W - eta*sum_grad    \n",
    "            \n",
    "                   \n",
    "        # Test accuracy at the end of each epoch  \n",
    "        ce_sum_t = 0\n",
    "        test_acc = 0\n",
    "        \n",
    "        for i in range (len(Xtest)):\n",
    "            target_test = targets_test[i]\n",
    "            ce,g,pred = f(W,Xtest[i],target_test)\n",
    "            \n",
    "            ce_sum_t += ce\n",
    "            #on regarde si la prediction vaut bien le label de notre cible\n",
    "            if (list(pred).index(np.max(pred)) == list(target_test).index(1)):\n",
    "                test_acc += 1\n",
    "        history[bs]['test acc'] += [test_acc/batch.shape[0]]\n",
    "        history[bs]['test loss'] += [ce_sum_t/batch.shape[0]]\n",
    "            \n",
    "        \n",
    "        print('Epoch number :', n+1,'test accuracy:',history[bs]['test acc'][n+1],'test loss',history[bs]['test loss'][n+1])\n",
    "        \n",
    "\n",
    "    print('\\n')            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the evolution of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in BatchSizes:\n",
    "       \n",
    "    n_batch = Xtrain.shape[0]//bs     \n",
    "    if Xtrain.shape[0]%bs!=0:\n",
    "        n_batch+=1\n",
    "        \n",
    "    E  = [n_batch*n for n in np.arange(N_EPOCHS+1)]\n",
    "    Ep = [str(n) for n in np.arange(N_EPOCHS+1)]\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(history[bs]['train loss'],label = 'training loss')\n",
    "    plt.plot(E[1:],history[bs]['test loss'][1:],linewidth=2.5,label = 'test loss')\n",
    "    plt.xticks(E,Ep)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss Value')\n",
    "    #plt.ylim([0,np.max(history[bs]['test loss'])+2])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(f'model trained with a Batch size of {bs} samples and learning rate of {eta}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of the evolution of the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bs in BatchSizes:\n",
    "    print(bs)   \n",
    "    n_batch = Xtrain.shape[0]//bs     \n",
    "    if Xtrain.shape[0]%bs!=0:\n",
    "        n_batch+=1\n",
    "        \n",
    "    print(n_batch)\n",
    "    E=[n_batch*n for n in np.arange(N_EPOCHS+1)]\n",
    "    Ep = [str(n) for n in np.arange(N_EPOCHS+1)]\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(history[bs]['train acc'] ,label  = 'training acuracy')\n",
    "    plt.plot(E[1:],history[bs]['test acc'][1:],linewidth=2.5,label = 'test acuracy')\n",
    "    plt.xticks(E,Ep)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.title(f'model trained with a Batch size of {bs} samples and learning rate of {eta}')\n",
    "    plt.ylim([0,1])\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results\n",
    "\n",
    "Please provide your comments on the sensitivity of the results to the parameters involved in the learning process (batch size, learning rate, momentum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
